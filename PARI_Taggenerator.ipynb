{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\vhasfcbhogas\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (2.18.4)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\vhasfcbhogas\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\vhasfcbhogas\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\users\\vhasfcbhogas\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vhasfcbhogas\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests) (2018.1.18)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\vhasfcbhogas\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (4.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VHASFCBHOGAS\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\VHASFCBHOGAS\\AppData\\Local\\Continuum\\anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "url = \"ruralindiaonline.org/articles/big-pharma-kills-small-fish-in-kovvada\"\n",
    "r  = requests.get(\"http://\" +url)\n",
    "data = r.text\n",
    "\n",
    "soup = BeautifulSoup(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = soup.find_all('p', attrs={'class':'MsoNormal'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = soup.findAll('p', attrs={'class':'MsoCommentText'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import urllib3\n",
    "url = \"http://ruralindiaonline.org/articles/big-pharma-kills-small-fish-in-kovvada\"\n",
    "#page = urllib3.urlopen(url)\n",
    "soup1 = BeautifulSoup(r.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm1 = soup.find_next('p', attrs={'class': 'MsoNormal'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better way to scrape all text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = []\n",
    "for p in soup.find_all('p'):\n",
    "    t = p.get_text().replace('\\n', '')\n",
    "    a = t.replace('\\xa0', '')\n",
    "    txt.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = txt[1]\n",
    "# t.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "alltxt = ''.join(str(elem) + ' ' for elem in txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\VHASFCBHOGAS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-03 14:52:13,687 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-10-03 14:52:13,690 : INFO : built Dictionary(534 unique tokens: [',', 'a', 'abundant', 'after', 'an']...) from 21 documents (total 1380 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "tokenized_txt = [word_tokenize(doc.lower()) for doc in txt]\n",
    "dictionary = Dictionary(tokenized_txt)\n",
    "\n",
    "token_alltxt = word_tokenize(alltxt.lower())\n",
    "# dict_alltxt = Dictionary(token_alltxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-03 14:52:14,460 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-10-03 14:52:14,460 : INFO : built Dictionary(540 unique tokens: [\"'s\", '(', ')', ',', '.']...) from 1 documents (total 1373 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "dict_alltxt = Dictionary([token_alltxt])\n",
    "# type(tokenized_txt[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary.token2id\n",
    "# dict_alltxt.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_txt]\n",
    "corpusalltxt = dictionary.doc2bow(token_alltxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(1464, 1477), match='areporttitled'>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "match = re.search( 'areporttitled' ,alltxt)\n",
    "match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "alltxt = re.sub('areporttitled', 'a report titled ', alltxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\VHASFCBHOGAS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens= [w for w in word_tokenize(alltxt.lower()) if w.isalpha()]  #toeknizing only alphabets. removing numerical etc data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['destroyed', 'fishing', 'in', 'this', 'village', 'in', 'andhra']"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[3:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stops = [t for t in tokens if t not in stopwords.words('english')]  # removed stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not lemmatized:  [('village', 12), ('says', 10), ('baskets', 9), ('one', 8), ('fish', 8), ('andhra', 7), ('hammocks', 7), ('industries', 6), ('fishing', 6), ('pradesh', 6)]\n",
      "Lemmatized:  [('village', 15), ('industry', 11), ('say', 11), ('basket', 9), ('hammock', 8), ('one', 8), ('fish', 8), ('andhra', 7), ('fishing', 6), ('pradesh', 6)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Not lemmatized: \", Counter(no_stops).most_common(10))\n",
    "print(\"Lemmatized: \", Counter(lemmatized).most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-03 17:30:11,756 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-10-03 17:30:11,756 : INFO : built Dictionary(382 unique tokens: ['abundant', 'accompanied', 'acre', 'across', 'act']...) from 1 documents (total 635 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "dict_alltxt = Dictionary([lemmatized])\n",
    "corpusalltxt = dictionary.doc2bow(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder \"C:\\Users\\VHASFC~2\\AppData\\Local\\Temp\\1\" will be used to save temporary dictionary and corpus.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "TEMP_FOLDER = tempfile.gettempdir()\n",
    "print('Folder \"{}\" will be used to save temporary dictionary and corpus.'.format(TEMP_FOLDER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['after',\n",
      "  'industries',\n",
      "  'fishing',\n",
      "  'village',\n",
      "  'andhra',\n",
      "  'pradesh,',\n",
      "  'who',\n",
      "  'have',\n",
      "  'other',\n",
      "  'options,',\n",
      "  'life',\n",
      "  '–',\n",
      "  'while'],\n",
      " ['time,',\n",
      "  'men',\n",
      "  'are',\n",
      "  'village.',\n",
      "  'market',\n",
      "  'at',\n",
      "  'others',\n",
      "  'market',\n",
      "  'or',\n",
      "  'them',\n",
      "  'selling',\n",
      "  'baskets',\n",
      "  'says',\n",
      "  'myalapilli',\n",
      "  'pattayya,',\n",
      "  'has',\n",
      "  'returned',\n",
      "  'from'],\n",
      " ['years',\n",
      "  'like',\n",
      "  'others',\n",
      "  'his',\n",
      "  'village,',\n",
      "  'started',\n",
      "  'making',\n",
      "  'nylon-rope',\n",
      "  'swings',\n",
      "  'hammocks',\n",
      "  'around',\n",
      "  '20',\n",
      "  'years',\n",
      "  'ago.',\n",
      "  'fishing',\n",
      "  'kovvada',\n",
      "  'small',\n",
      "  'village',\n",
      "  'around',\n",
      "  'ranastalam',\n",
      "  'srikakulam'],\n",
      " ['pollution',\n",
      "  'started',\n",
      "  'aquatic',\n",
      "  'wealth',\n",
      "  'industries',\n",
      "  'up',\n",
      "  'pydibhimavaram',\n",
      "  'village',\n",
      "  '10',\n",
      "  'kilometres',\n",
      "  'they',\n",
      "  'have',\n",
      "  'as',\n",
      "  'as'],\n",
      " ['environment',\n",
      "  'pharmaceutical',\n",
      "  'manufacturing',\n",
      "  'as',\n",
      "  'activity',\n",
      "  'after',\n",
      "  'pharma',\n",
      "  'began',\n",
      "  'from',\n",
      "  'industry',\n",
      "  'has',\n",
      "  'pharmaceutical',\n",
      "  'pollution',\n",
      "  'environment',\n",
      "  'are',\n",
      "  'andhra',\n",
      "  'as',\n",
      "  'pharmaceutical',\n",
      "  'andhra'],\n",
      " ['myalapilli',\n",
      "  '(right)',\n",
      "  'other',\n",
      "  'village',\n",
      "  'they',\n",
      "  'make',\n",
      "  'baskets',\n",
      "  'hammocks'],\n",
      " ['region',\n",
      "  'is',\n",
      "  'now',\n",
      "  'pharma',\n",
      "  'andhra',\n",
      "  'pradesh,',\n",
      "  'with',\n",
      "  'industries',\n",
      "  'national',\n",
      "  'industry',\n",
      "  'got',\n",
      "  'here',\n",
      "  'when',\n",
      "  'began',\n",
      "  'up',\n",
      "  'their',\n",
      "  'here',\n",
      "  'industries',\n",
      "  'with',\n",
      "  'are',\n",
      "  'andhra',\n",
      "  'pradesh,',\n",
      "  'including',\n",
      "  'four',\n",
      "  '–',\n",
      "  'pydibhimavaram',\n",
      "  'is',\n",
      "  'one',\n",
      "  'these',\n",
      "  '–',\n",
      "  'manufacturing',\n",
      "  'pharma'],\n",
      " ['are',\n",
      "  '15',\n",
      "  'kilometres',\n",
      "  'into',\n",
      "  'but',\n",
      "  'from',\n",
      "  'pharma',\n",
      "  'industries',\n",
      "  'are',\n",
      "  'seen',\n",
      "  'up',\n",
      "  '100',\n",
      "  'kilometres',\n",
      "  'from',\n",
      "  'coast',\n",
      "  'we',\n",
      "  'go',\n",
      "  'says',\n",
      "  'ganagalla',\n",
      "  'ramudu,',\n",
      "  'who',\n",
      "  'one',\n",
      "  'village',\n",
      "  'had',\n",
      "  'at',\n",
      "  'years',\n",
      "  'ago.',\n",
      "  'only',\n",
      "  '10',\n",
      "  'are',\n",
      "  'he',\n",
      "  '“we',\n",
      "  'three',\n",
      "  'but',\n",
      "  'no',\n",
      "  'one',\n",
      "  'so',\n",
      "  'we',\n",
      "  'back',\n",
      "  'our'],\n",
      " ['“the',\n",
      "  'aquatic',\n",
      "  'wealth',\n",
      "  'region',\n",
      "  'got',\n",
      "  'because',\n",
      "  'pollution',\n",
      "  'by',\n",
      "  'pharmaceutical',\n",
      "  'turtles',\n",
      "  'fish',\n",
      "  'are',\n",
      "  'seen',\n",
      "  'coast',\n",
      "  'these',\n",
      "  'plant',\n",
      "  'life',\n",
      "  'is',\n",
      "  'which',\n",
      "  'has',\n",
      "  'aquatic',\n",
      "  'says',\n",
      "  'based',\n",
      "  'village,',\n",
      "  'who',\n",
      "  'with',\n",
      "  'national'],\n",
      " ['ganagalla',\n",
      "  'says',\n",
      "  'pharma',\n",
      "  'industry',\n",
      "  'seen',\n",
      "  '100',\n",
      "  'kilometres',\n",
      "  'from',\n",
      "  'they',\n",
      "  'fish',\n",
      "  'turtles',\n",
      "  'up',\n",
      "  'shore'],\n",
      " ['has',\n",
      "  'made',\n",
      "  'fishing',\n",
      "  'activity',\n",
      "  'kovvada',\n",
      "  'other',\n",
      "  '“we',\n",
      "  'no',\n",
      "  'go',\n",
      "  'fishing',\n",
      "  'because',\n",
      "  'we',\n",
      "  'catch',\n",
      "  'fish',\n",
      "  'even',\n",
      "  'after',\n",
      "  'says',\n",
      "  'myalapilli',\n",
      "  'appanna.',\n",
      "  '“we',\n",
      "  'go',\n",
      "  'into',\n",
      "  'at',\n",
      "  '20',\n",
      "  'at',\n",
      "  'before',\n",
      "  'back',\n",
      "  'shore',\n",
      "  'at',\n",
      "  'or',\n",
      "  'four',\n",
      "  'go',\n",
      "  'we',\n",
      "  'even',\n",
      "  'make',\n",
      "  '100',\n",
      "  'rupees',\n",
      "  'per'],\n",
      " ['“the',\n",
      "  'fish',\n",
      "  'we',\n",
      "  'catch',\n",
      "  'is',\n",
      "  'not',\n",
      "  'even',\n",
      "  'our',\n",
      "  'selling',\n",
      "  'making',\n",
      "  'we',\n",
      "  'have',\n",
      "  'get',\n",
      "  'fish',\n",
      "  'from',\n",
      "  'visakhapatnam,',\n",
      "  'srikakulam',\n",
      "  'or',\n",
      "  'ranastalam',\n",
      "  'our',\n",
      "  'adds'],\n",
      " ['so',\n",
      "  'appanna',\n",
      "  'pattayya,',\n",
      "  'like',\n",
      "  'many',\n",
      "  'others',\n",
      "  'kovvada,',\n",
      "  'turned',\n",
      "  'making',\n",
      "  'baskets,',\n",
      "  'swings',\n",
      "  'which',\n",
      "  'they',\n",
      "  'sells',\n",
      "  'across',\n",
      "  'they',\n",
      "  'options,',\n",
      "  'one',\n",
      "  'turned',\n",
      "  'out',\n",
      "  'they',\n",
      "  'nylon',\n",
      "  '“i',\n",
      "  'have',\n",
      "  'across',\n",
      "  'them',\n",
      "  '20',\n",
      "  'says',\n",
      "  'appanna.',\n",
      "  'his',\n",
      "  '“i',\n",
      "  'make',\n",
      "  'baskets',\n",
      "  'while',\n",
      "  'them',\n",
      "  'other',\n",
      "  'places',\n",
      "  'sell'],\n",
      " ['one',\n",
      "  'kilo',\n",
      "  'nylon-rope',\n",
      "  'costs',\n",
      "  'rs.',\n",
      "  'including',\n",
      "  'costs',\n",
      "  'by',\n",
      "  'or',\n",
      "  'village.',\n",
      "  '“we',\n",
      "  'make',\n",
      "  'baskets',\n",
      "  'from',\n",
      "  'one',\n",
      "  'kilo',\n",
      "  'sell',\n",
      "  'each',\n",
      "  'rs.',\n",
      "  '10',\n",
      "  'making',\n",
      "  'profit',\n",
      "  'rs.',\n",
      "  'per',\n",
      "  'adds',\n",
      "  'appanna.',\n",
      "  'hammocks',\n",
      "  'or',\n",
      "  'swings',\n",
      "  'are',\n",
      "  'made',\n",
      "  'nylon',\n",
      "  'each',\n",
      "  'one',\n",
      "  'sells',\n",
      "  'rs.'],\n",
      " [],\n",
      " ['now', 'make', 'nylon-rope', 'seen', 'here', 'appanna', '(right)'],\n",
      " ['men',\n",
      "  'village',\n",
      "  'go',\n",
      "  'places',\n",
      "  'sell',\n",
      "  'ganagalla',\n",
      "  'ramudu,',\n",
      "  'who',\n",
      "  'him',\n",
      "  'their',\n",
      "  'costs',\n",
      "  'while',\n",
      "  'i',\n",
      "  'returned',\n",
      "  '15',\n",
      "  'i',\n",
      "  'had',\n",
      "  'only'],\n",
      " ['have',\n",
      "  'made',\n",
      "  'him',\n",
      "  '“we',\n",
      "  'up',\n",
      "  'we',\n",
      "  'go',\n",
      "  'is',\n",
      "  'important',\n",
      "  'with',\n",
      "  'our',\n",
      "  'he',\n",
      "  'are',\n",
      "  'now',\n",
      "  'when',\n",
      "  'village',\n",
      "  'men',\n",
      "  'who',\n",
      "  'have',\n",
      "  'out',\n",
      "  'selling',\n",
      "  'baskets',\n",
      "  'hammocks',\n",
      "  'back',\n",
      "  'important',\n",
      "  'they',\n",
      "  'are'],\n",
      " ['like',\n",
      "  'many',\n",
      "  'village,',\n",
      "  'making',\n",
      "  'baskets,',\n",
      "  'hammocks',\n",
      "  'mgnrega',\n",
      "  'which',\n",
      "  'them',\n",
      "  '“i',\n",
      "  'four',\n",
      "  'weeks',\n",
      "  'but',\n",
      "  'got',\n",
      "  'only',\n",
      "  'two',\n",
      "  'weeks',\n",
      "  'at',\n",
      "  '100',\n",
      "  'rupees',\n",
      "  'per',\n",
      "  'says',\n",
      "  'who',\n",
      "  'sells',\n",
      "  'fish',\n",
      "  'mgnrega',\n",
      "  'is',\n",
      "  'rs.',\n",
      "  'andhra',\n",
      "  'pradesh.',\n",
      "  '“we',\n",
      "  'get',\n",
      "  'fish',\n",
      "  'from',\n",
      "  'visakhapatnam,',\n",
      "  'them',\n",
      "  'two',\n",
      "  'before',\n",
      "  'selling',\n",
      "  'time,',\n",
      "  'we',\n",
      "  'got',\n",
      "  'these',\n",
      "  'fish',\n",
      "  'we',\n",
      "  'have',\n",
      "  'rupees',\n",
      "  'get',\n",
      "  'profit',\n",
      "  'says'],\n",
      " ['after',\n",
      "  'even',\n",
      "  'small',\n",
      "  'profit',\n",
      "  'not',\n",
      "  'plant',\n",
      "  'across',\n",
      "  'three',\n",
      "  'including',\n",
      "  'kovvada,',\n",
      "  'two',\n",
      "  'their',\n",
      "  'baskets',\n",
      "  'fishing'],\n",
      " ['is', 'based', 'andhra', 'pradesh.']]\n"
     ]
    }
   ],
   "source": [
    "# remove common words and tokenize\n",
    "stoplist = set('for a an of be that on the this and to in'.split())\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "         for document in documents]\n",
    "\n",
    "# remove words that appear only once\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "\n",
    "from pprint import pprint  # pretty-printer\n",
    "pprint(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
